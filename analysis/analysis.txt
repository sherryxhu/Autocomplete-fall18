Put your name and netid here

(1) Run the program BenchmarkForAutocomplete and copy/paste the 
results here this for #matches = 20

search	size	#match	binary	brute
	456976	20	0.2085	0.0110
a	17576	20	0.0041	0.0108
b	17576	20	0.0047	0.0092
c	17576	20	0.0041	0.0081
x	17576	20	0.0040	0.0041
y	17576	20	0.0060	0.0043
z	17576	20	0.0039	0.0054
aa	676		20	0.0001	0.0042
az	676		20	0.0002	0.0040
za	676		20	0.0002	0.0042
zz	676		20	0.0001	0.0043

(2) Run the program again for #matches = 10000, paste the results, 
and then make any conclusions about how the # matches 
effects the runtime. 

search	size	#match	binary	brute
	456976	10000	0.2251	0.0316
a	17576	10000	0.0274	0.0164
b	17576	10000	0.0058	0.0103
c	17576	10000	0.0050	0.0107
x	17576	10000	0.0074	0.0105
y	17576	10000	0.0054	0.0115
z	17576	10000	0.0061	0.0123
aa	676		10000	0.0001	0.0063
az	676		10000	0.0003	0.0069
za	676		10000	0.0001	0.0060
zz	676		10000	0.0002	0.0059

From the output of the code, it looks like the number of matches does not really affect the runtime. For example, 
for az, za, and zz (last three), there is barely any difference in the runtime for binary search. The runtimes all differ by
0.0001 seconds (0.0002 and 0.0003, 0.0002 and 0.0001, and 0.0001 and 0.0002 respectively. Similarly, under the brute column, the
times all differ very trivially (0.0040 and 0.0069, 0.0042 and 0.0060, and 0.0043 and 0.0059 respectively). 

(3) Copy/paste the code from BruteAutocomplete.topMatches below. 
Explain what the Big-Oh complexity of the entire loop: 
for(Term t : myTerms) {...} 
is in terms of N, the number of elements in myTerms and 
M, the number of terms that match the prefix. 
Assume that every priority-queue operation runs in O(log k) time. 
Explain your answer which should be in terms of N, M, and k.

public List<Term> topMatches(String prefix, int k) {
		if (k < 0) {
			throw new IllegalArgumentException("Illegal value of k:"+k);
		}
		
		// maintain pq of size k
		PriorityQueue<Term> pq = new PriorityQueue<Term>(10, new Term.WeightOrder());
		for (Term t : myTerms) {
			if (!t.getWord().startsWith(prefix))
				continue;
			if (pq.size() < k) {
				pq.add(t);
			} else if (pq.peek().getWeight() < t.getWeight()) {
				pq.remove();
				pq.add(t);
			}
		}
		int numResults = Math.min(k, pq.size());
		LinkedList<Term> ret = new LinkedList<>();
		for (int i = 0; i < numResults; i++) {
			ret.addFirst(pq.remove());
		}
		return ret;
	}
	
The runtime is O(N + Mlogk). This is because the pq operations only happens when there is a match. Each pq operation is
O(log k) and there are M matches, hence O(Mlogk). There are N elements in myTerms, and the loop runs N times, with at least
an O(1) runtime within each iteration of the for loop. 

(4) Explain why the last for loop in BruteAutocomplete.topMatches 
uses a LinkedList (and not an ArrayList) 
AND why the PriorityQueue uses Term.WeightOrder to get 
the top k heaviest matches -- rather than 
using Term.ReverseWeightOrder.

Inserting an element to the beginning of a LinkedList is O(1) so for N elements this would be O(N). Inserting an element 
to the beginning of an ArrayList requires you to shift every element down, meaning for N elements it would take O(N^2) time.

The pq uses Term.WeightOrder because the pq is a min heap and the term with least weight would end up on top. Then when you add 
to the front of a linked list, the first element will be the term with the heaviest weight.

(5) Explain what the runtime of the 
BinarySearchAutocomplete.topMatches code that you 
implemented is by copy/pasting the code below 
and explaining your answer in terms of N, M, and k.

public List<Term> topMatches(String prefix, int k) {
		
		ArrayList<Term> list = new ArrayList<>();
		int firstIndex = firstIndexOf(myTerms, new Term(prefix, 0), new Term.PrefixOrder(prefix.length()));
		int lastIndex = lastIndexOf(myTerms, new Term(prefix, 0), new Term.PrefixOrder(prefix.length()));
		
		if(firstIndex==-1) {
			return new ArrayList<>();
		}
		
		for(int i = firstIndex; i<= lastIndex; i++) {
			list.add(myTerms[i]);
		}
		Collections.sort(list, new Term.ReverseWeightOrder());
		ArrayList<Term> ansr = new ArrayList<>();
		
		for(int j = 0; j<k && j<list.size(); j++) {
			ansr.add(list.get(j));
		}
		return ansr;
	}

	
The answer is O(logN + MlogM + k). Calling firstIndex and lastIndex is O(logN) because it is binary search. We then add M 
matches to list, which is O(M). Sorting that list is O(MlogM), so O(M) can be disregarded because we only take the
highest order, leaving us with O(MlogM). Then we add k elements to a new list called ansr, which is O(k). 